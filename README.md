>**About**

 `rcognita` is a framework for dynamic programming and reinforcement learning algorithm development, testing, and simulation. The framework offers a playground of ready presets consisting of agents (controllers) and environments (dynamical systems). The core of the framework is `regelum` - a flexible universe of interconnected modules realizing a learn-in-the-loop concept, i.e., learning on the fly via data generated by the system. A distinct feature of `regelum` is that the controllers and systems exist in the same universe, allowing one to assemble a single preset combining both types of entities.

> **Table of Contents**
- [Setup](#setup)
- [Getting Started](#getting-started)
  - [Simulation in Gazebo](#simulation-in-gazebo)
  - [Perform learning process in Gazebo](#perform-learning-process-in-gazebo)
    - [1. CALF](#1-calf)
    - [2. SARSA-m](#2-sarsa-m)
    - [3. PPO](#3-ppo)


# Setup
[Installation guildlines](docs/Installation.md)

# Getting Started
This repository is aimed to perform all the tasks in a docker container. Details of running and attaching the docker container can be found [here](docs/Installation.md#3-run-docker).

**ALL THE FOLLOWING COMMANDS HAVE TO BE USED INSIDE DOCKER CONTAINTER.**

## Simulation in Gazebo

To launch Turtlebot3 simulation in Gazebo

``` bash
roslaunch turtlebot3_gazebo turtlebot3_empty_world.launch
```

## Perform learning process in Gazebo

Open another Terminal, to navigate to the workspace folder inside docker.

``` bash
cd regelum-ws
```

### 1. CALF

Execute the predefined script (for 20 seeds) by using the following command:

```
source scripts/launch_calf.sh -r
```

or directly execute with the seed 7:

```
python3.10 run.py \
           +seed=7 \
           simulator=ros \
           policy=rc_calfq \
           initial_conditions=3wrobot_kin_with_spot \
           +policy.nominal_kappa_params="[0.2, 1.5, -.15]" \
           scenario=my_scenario \
           system=3wrobot_kin_with_spot \
           common.sampling_time=0.1 \
           simulator.time_final=40 \scenario.N_iterations=40 \
           --single-thread \
           --experiment=calf_report \
           policy.critic_desired_decay=1e-6 \
           policy.critic_low_kappa_coeff=1e-1 \
           policy.critic_up_kappa_coeff=1e3 \
           policy.penalty_factor=1e2 \
           policy.step_size_multiplier=5 \
           policy.nominal_only=False
```

### 2. SARSA-m

Execute the predefined script (for 20 seeds) by using the following command:

```
source scripts/launch_sarsa_m.sh -r
```

or directly execute with the seed 7:

```
python3.10 run.py \
           +seed=7 \
           simulator=ros \
           policy=rc_sarsa_m \
           initial_conditions=3wrobot_kin_with_spot \
           +policy.R1_diag="[100, 100, 1, 0, 0]" \
           scenario=my_scenario \
           system=3wrobot_kin_with_spot \
           common.sampling_time=0.1 \
           simulator.time_final=50 \
           scenario.N_iterations=50 \
           --single-thread \
           --jobs=-1 \
           --experiment=sarsa_m_report \
           policy.critic_desired_decay=1e-6 \
           policy.critic_low_kappa_coeff=1e-1 \
           policy.critic_up_kappa_coeff=5e2 \
           policy.penalty_factor=1e2 \
           policy.step_size_multiplier=5
```

### 3. PPO
   
Execute the predefined script (for 20 seeds) by using the following command:

```
source scripts/launch_ppo.sh -r
```

or directly execute with the seed 7:

```
python3.10 run.py \
           +seed=7 \
           simulator=ros \
           scenario=ppo_scenario \
           system=3wrobot_kin_customized \
           --single-thread \
           --experiment=ppo_3wrobot_kin_retry_2708_1 \
           scenario.N_episodes=1 \
           scenario.N_iterations=100 \
           scenario.policy_n_epochs=50 \
           scenario.critic_n_epochs=50 \
           scenario.policy_opt_method_kwargs.lr=0.005 \
           scenario.policy_model.n_hidden_layers=2 \
           scenario.policy_model.dim_hidden=15 \
           scenario.policy_model.std=0.1 \
           scenario.critic_model.n_hidden_layers=3 \
           scenario.critic_model.dim_hidden=15 \
           scenario.critic_opt_method_kwargs.lr=0.1 \
           scenario.gae_lambda=0 \
           scenario.discount_factor=0.9 \
           scenario.cliprange=0.2 \
           scenario.critic_td_n=1 \
           simulator.time_final=30 \
           common.sampling_time=0.1
```

